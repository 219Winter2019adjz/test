{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ZUIDFL5jxvxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7FSvO5_xvxj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
        "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
        "              'rec.autos', 'rec.motorcycles',\n",
        "              'rec.sport.baseball', 'rec.sport.hockey']\n",
        "train_dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "test_dataset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ob_91Lq_xvxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 1"
      ]
    },
    {
      "metadata": {
        "id": "0WEfap6yxvxm",
        "colab_type": "code",
        "outputId": "950285b7-1ec3-49f3-d92b-cf2686238fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "# Fetch all 20 news groups categories and plot a histogram of the training documents.\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "plt.hist(newsgroups_train.target, 20)\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "aIGvgdKMxvxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 2"
      ]
    },
    {
      "metadata": {
        "id": "PCUxRNaDxvxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Fetching 20NewsGroups dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Refer to the offcial document of scikit-learn for detailed usages:\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
        "categories = ['comp.graphics', 'comp.sys.mac.hardware']\n",
        "twenty_train = fetch_20newsgroups(subset='train', # choose which subset of the dataset to use; can be 'train', 'test', 'all'\n",
        "                                  categories=categories, # choose the categories to load; if is `None`, load all categories\n",
        "                                  shuffle=True,\n",
        "                                  random_state=42, # set the seed of random number generator when shuffling to make the outcome repeatable across different runs\n",
        "                                  # remove=['headers'],\n",
        "                                  )\n",
        "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1Aq6iCbxvxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Perform lemmatization on dataset\n",
        "\n",
        "# The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "# nltk.download('punkt')#, if you need \"tokenizers/punkt/english.pickle\", choose it\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
        "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
        "                  'VB': 'v', 'RB': 'r'}\n",
        "    try:\n",
        "        return morphy_tag[penntag[:2]]\n",
        "    except:\n",
        "        return 'n'\n",
        "\n",
        "\n",
        "# def lemmatize_sent(list_word, wnl):\n",
        "#     # Text input is string, returns array of lowercased strings(words).\n",
        "#     return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
        "#             for word, tag in pos_tag(list_word)]\n",
        "\n",
        "\n",
        "wnl = nltk.wordnet.WordNetLemmatizer()\n",
        "def lemmatize_training(text):\n",
        "    # Text input is string, returns array of lowercased strings(words).\n",
        "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
        "            for word, tag in pos_tag(nltk.word_tokenize(text))]\n",
        "\n",
        "\n",
        "# TODO: should this filter out the following numbers too? \"4-5\" \"c650\"\n",
        "def filter_numbers(text_array):\n",
        "    # Filter out any numbers found in the array of strings\n",
        "    output = []\n",
        "    for s in text_array:\n",
        "        if not s.isdigit():\n",
        "            # if not a digit...\n",
        "            try:\n",
        "                # if a float, filter out\n",
        "                float(s)\n",
        "            except ValueError:\n",
        "                # if not a float, add to output\n",
        "                output.append(s)\n",
        "        else:\n",
        "            # if a digit, filter out\n",
        "            pass\n",
        "    return output\n",
        "\n",
        "\n",
        "def array_to_string(text_array, delimeter=\"\"):\n",
        "    # Converts an array back into a string of words using the provided delimeter to add between each word\n",
        "    output = \"\"\n",
        "    for s in text_array:\n",
        "        output = output + delimeter + s\n",
        "    return output\n",
        "\n",
        "\n",
        "def lemmatize_and_filter(documents):\n",
        "    # Performs lemmatization, and number filtering on the given documents\n",
        "    lemmatized_data = []\n",
        "    for i in documents:\n",
        "        # lemmatize the document:\n",
        "        training_tagged = pos_tag(nltk.word_tokenize(i))\n",
        "        lemmatized_array = lemmatize_training(i)\n",
        "\n",
        "        # remove numbers from document:\n",
        "        filtered_array = filter_numbers(lemmatized_array)\n",
        "\n",
        "        # reassemble back to string:\n",
        "        lemmatized_string = array_to_string(filtered_array, ' ')\n",
        "\n",
        "        # add to final data list\n",
        "        # print(lemmatized_string)\n",
        "        lemmatized_data.append(lemmatized_string)\n",
        "\n",
        "    return lemmatized_data\n",
        "\n",
        "\n",
        "# print(lemmatized_data[0])\n",
        "lemmatized_training = lemmatize_and_filter(twenty_train.data)\n",
        "lemmatized_testing = lemmatize_and_filter(twenty_test.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BjA770b7xvxw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Push lemmatized documents through CountVectorizer\n",
        "\n",
        "# count_vect = CountVectorizer(min_df=3)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# do for training\n",
        "count_vect = CountVectorizer(min_df=3, stop_words='english')\n",
        "X_lemmatized_train_counts = count_vect.fit_transform(lemmatized_training)\n",
        "\n",
        "# do for testing\n",
        "X_lemmatized_test_counts = count_vect.transform(lemmatized_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0Ju4BQ8xvxz",
        "colab_type": "code",
        "outputId": "3b08a35f-cb57-4a24-81de-f5aaa28c9981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2159
        }
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Report shapes of TF-IDF matrices\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# do for training\n",
        "X_lemmatized_train_tfidf = tfidf_transformer.fit_transform(X_lemmatized_train_counts)\n",
        "\n",
        "print(X_lemmatized_train_tfidf.shape)\n",
        "print('-' * 20)\n",
        "print(X_lemmatized_train_counts.toarray()[:30, :5])\n",
        "print('-' * 20)\n",
        "print(X_lemmatized_train_tfidf.toarray()[:30, :5])\n",
        "\n",
        "# do for testing\n",
        "X_lemmatized_test_tfidf = tfidf_transformer.transform(X_lemmatized_test_counts)\n",
        "\n",
        "print(X_lemmatized_test_tfidf.shape)\n",
        "print('-' * 20)\n",
        "print(X_lemmatized_test_counts.toarray()[:30, :5])\n",
        "print('-' * 20)\n",
        "print(X_lemmatized_test_tfidf.toarray()[:30, :5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1162, 5167)\n",
            "--------------------\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "--------------------\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.12043498 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]]\n",
            "(774, 5167)\n",
            "--------------------\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "--------------------\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.14204286 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "5o5ftdgBxvx3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 3"
      ]
    },
    {
      "metadata": {
        "id": "MVuiiQJoxvx3",
        "colab_type": "code",
        "outputId": "f6f0eb5d-3323-40d3-e0e7-c19cc6f64dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform LSI using the truncated SVD\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_lsi_train_reduced = svd.fit_transform(X_lemmatized_train_tfidf)\n",
        "Y_lsi_train_reduced = svd.components_\n",
        "print(X_lsi_train_reduced.shape)\n",
        "print(svd.components_.shape)\n",
        "\n",
        "X_lsi_test_reduced = svd.fit_transform(X_lemmatized_test_tfidf)\n",
        "Y_lsi_test_reduced = svd.components_\n",
        "print(X_lsi_train_reduced.shape)\n",
        "print(svd.components_.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1162, 50)\n",
            "(50, 5167)\n",
            "(1162, 50)\n",
            "(50, 5167)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "URSfInKOxvx6",
        "colab_type": "code",
        "outputId": "359600de-1955-40d0-f173-d8666ab114c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform NMF\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "model = NMF(n_components=50, init='random', random_state=42)\n",
        "W_nmf_train_reduced = model.fit_transform(X_lemmatized_train_tfidf)\n",
        "H_nmf_train_reduced = model.components_\n",
        "\n",
        "print(W_nmf_train_reduced.shape)\n",
        "print(H_nmf_train_reduced.shape)\n",
        "\n",
        "W_nmf_test_reduced = model.fit_transform(X_lemmatized_test_tfidf)\n",
        "H_nmf_test_reduced = model.components_"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1162, 50)\n",
            "(50, 5167)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AfGbsKGhxvx-",
        "colab_type": "code",
        "outputId": "d67741f8-02a4-4636-8a52-128cf1bfbcf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Compare LSI and NMF\n",
        "\n",
        "nmf_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(W_nmf_train_reduced, H_nmf_train_reduced), 'fro')\n",
        "lsi_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(X_lsi_train_reduced, Y_lsi_train_reduced), 'fro')\n",
        "\n",
        "print('NMF: ', nmf_val)\n",
        "print('LSI: ', lsi_val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NMF:  30.107267737626056\n",
            "LSI:  29.800199080327733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UGNn6K_exvyB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Question 6"
      ]
    },
    {
      "metadata": {
        "id": "5NkfS2HZxvyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Train a Naive Bayes Gaussian classifier on the reduced TFIDF training set from problem 3\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = GaussianNB().fit(W_nmf_train_reduced, twenty_train.target)\n",
        "clf2 = MultinomialNB().fit(W_nmf_train_reduced, twenty_train.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQN9jIzvxvyE",
        "colab_type": "code",
        "outputId": "430ee928-6d79-4f36-b790-d83c193c62fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Generate predictions for test set\n",
        "\n",
        "predicted = clf.predict(W_nmf_test_reduced)\n",
        "correct = 0\n",
        "for i, category in enumerate(predicted):\n",
        "    if category == twenty_test.target[i]:\n",
        "        correct += 1\n",
        "    if i < 5:\n",
        "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
        "    elif i == 5:\n",
        "        print('...\\n')\n",
        "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics =? comp.graphics\n",
            "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
            "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
            "comp.graphics =? comp.sys.mac.hardware\n",
            "comp.graphics =? comp.sys.mac.hardware\n",
            "...\n",
            "\n",
            "Accuracy of NB Gaussian: 0.5568475452196382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h0IChMnAxvyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "55c5e1e4-3a8a-4d4f-d6eb-3ee1bae47fa5"
      },
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "# Generate predictions for test set\n",
        "\n",
        "predicted = clf2.predict(W_nmf_test_reduced)\n",
        "correct = 0\n",
        "for i, category in enumerate(predicted):\n",
        "    if category == twenty_test.target[i]:\n",
        "        correct += 1\n",
        "    if i < 5:\n",
        "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
        "    elif i == 5:\n",
        "        print('...\\n')\n",
        "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics =? comp.graphics\n",
            "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
            "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
            "comp.graphics =? comp.sys.mac.hardware\n",
            "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
            "...\n",
            "\n",
            "Accuracy of NB Gaussian: 0.5775193798449613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wp2QQX5ByD0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### TEST here"
      ]
    }
  ]
}