{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/219Winter2019adjz/Project1/blob/master/proj1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZUIDFL5jxvxe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "D7FSvO5_xvxj"
   },
   "outputs": [],
   "source": [
    "categories = ['comp.graphics', 'comp.os.ms-windows.misc',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'rec.autos', 'rec.motorcycles',\n",
    "              'rec.sport.baseball', 'rec.sport.hockey']\n",
    "train_dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "test_dataset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ob_91Lq_xvxl"
   },
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WEfap6yxvxm",
    "outputId": "dc8a8680-a3b7-420a-bb63-cb2aaf14810e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1082e84a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch all 20 news groups categories and plot a histogram of the training documents.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "plt.hist(newsgroups_train.target, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIGvgdKMxvxr"
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PCUxRNaDxvxs"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Fetching 20NewsGroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Refer to the offcial document of scikit-learn for detailed usages:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "categories = ['comp.graphics', 'comp.sys.mac.hardware']\n",
    "twenty_train = fetch_20newsgroups(subset='train', # choose which subset of the dataset to use; can be 'train', 'test', 'all'\n",
    "                                  categories=categories, # choose the categories to load; if is `None`, load all categories\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42, # set the seed of random number generator when shuffling to make the outcome repeatable across different runs\n",
    "                                  # remove=['headers'],\n",
    "                                  )\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y1Aq6iCbxvxu"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Perform lemmatization on dataset\n",
    "\n",
    "# The lemmatizer is actually pretty complicated, it needs Parts of Speech (POS) tags\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "# nltk.download('punkt')#, if you need \"tokenizers/punkt/english.pickle\", choose it\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "# def lemmatize_sent(list_word, wnl):\n",
    "#     # Text input is string, returns array of lowercased strings(words).\n",
    "#     return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "#             for word, tag in pos_tag(list_word)]\n",
    "\n",
    "\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "def lemmatize_training(text):\n",
    "    # Text input is string, returns array of lowercased strings(words).\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "            for word, tag in pos_tag(nltk.word_tokenize(text))]\n",
    "\n",
    "\n",
    "# TODO: should this filter out the following numbers too? \"4-5\" \"c650\"\n",
    "def filter_numbers(text_array):\n",
    "    # Filter out any numbers found in the array of strings\n",
    "    output = []\n",
    "    for s in text_array:\n",
    "        if not s.isdigit():\n",
    "            # if not a digit...\n",
    "            try:\n",
    "                # if a float, filter out\n",
    "                float(s)\n",
    "            except ValueError:\n",
    "                # if not a float, add to output\n",
    "                output.append(s)\n",
    "        else:\n",
    "            # if a digit, filter out\n",
    "            pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def array_to_string(text_array, delimeter=\"\"):\n",
    "    # Converts an array back into a string of words using the provided delimeter to add between each word\n",
    "    output = \"\"\n",
    "    for s in text_array:\n",
    "        output = output + delimeter + s\n",
    "    return output\n",
    "\n",
    "\n",
    "def lemmatize_and_filter(documents):\n",
    "    # Performs lemmatization, and number filtering on the given documents\n",
    "    lemmatized_data = []\n",
    "    for i in documents:\n",
    "        # lemmatize the document:\n",
    "        training_tagged = pos_tag(nltk.word_tokenize(i))\n",
    "        lemmatized_array = lemmatize_training(i)\n",
    "\n",
    "        # remove numbers from document:\n",
    "        filtered_array = filter_numbers(lemmatized_array)\n",
    "\n",
    "        # reassemble back to string:\n",
    "        lemmatized_string = array_to_string(filtered_array, ' ')\n",
    "\n",
    "        # add to final data list\n",
    "        # print(lemmatized_string)\n",
    "        lemmatized_data.append(lemmatized_string)\n",
    "\n",
    "    return lemmatized_data\n",
    "\n",
    "\n",
    "# print(lemmatized_data[0])\n",
    "lemmatized_training = lemmatize_and_filter(twenty_train.data)\n",
    "lemmatized_testing = lemmatize_and_filter(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BjA770b7xvxw"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Push lemmatized documents through CountVectorizer\n",
    "\n",
    "# count_vect = CountVectorizer(min_df=3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# do for training\n",
    "count_vect = CountVectorizer(min_df=3, stop_words='english')\n",
    "X_lemmatized_train_counts = count_vect.fit_transform(lemmatized_training)\n",
    "\n",
    "# do for testing\n",
    "X_lemmatized_test_counts = count_vect.transform(lemmatized_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0Ju4BQ8xvxz",
    "outputId": "11b41fa6-8266-401a-90b7-f27aaf7151c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 5167)\n",
      "--------------------\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "--------------------\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.12043498 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n",
      "(774, 5167)\n",
      "--------------------\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "--------------------\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.14204286 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Report shapes of TF-IDF matrices\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# do for training\n",
    "X_lemmatized_train_tfidf = tfidf_transformer.fit_transform(X_lemmatized_train_counts)\n",
    "\n",
    "print(X_lemmatized_train_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_train_counts.toarray()[:30, :5])\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_train_tfidf.toarray()[:30, :5])\n",
    "\n",
    "# do for testing\n",
    "X_lemmatized_test_tfidf = tfidf_transformer.transform(X_lemmatized_test_counts)\n",
    "\n",
    "print(X_lemmatized_test_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_test_counts.toarray()[:30, :5])\n",
    "print('-' * 20)\n",
    "print(X_lemmatized_test_tfidf.toarray()[:30, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "5o5ftdgBxvx3"
   },
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVuiiQJoxvx3",
    "outputId": "a5da2dcb-8bc2-45d9-d9b2-aa949a9c6ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 50)\n",
      "(50, 5167)\n",
      "(1162, 50)\n",
      "(50, 5167)\n"
     ]
    }
   ],
   "source": [
    "# Perform LSI using the truncated SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_lsi_train_reduced = svd.fit_transform(X_lemmatized_train_tfidf)\n",
    "Y_lsi_train_reduced = svd.components_\n",
    "print(X_lsi_train_reduced.shape)\n",
    "print(svd.components_.shape)\n",
    "\n",
    "X_lsi_test_reduced = svd.fit_transform(X_lemmatized_test_tfidf)\n",
    "Y_lsi_test_reduced = svd.components_\n",
    "print(X_lsi_train_reduced.shape)\n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URSfInKOxvx6",
    "outputId": "d93fc1e9-0aac-4211-e920-12872703dde2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 50)\n",
      "(50, 5167)\n"
     ]
    }
   ],
   "source": [
    "# Perform NMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=50, init='random', random_state=42)\n",
    "W_nmf_train_reduced = model.fit_transform(X_lemmatized_train_tfidf)\n",
    "H_nmf_train_reduced = model.components_\n",
    "\n",
    "print(W_nmf_train_reduced.shape)\n",
    "print(H_nmf_train_reduced.shape)\n",
    "\n",
    "W_nmf_test_reduced = model.fit_transform(X_lemmatized_test_tfidf)\n",
    "H_nmf_test_reduced = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfGbsKGhxvx-",
    "outputId": "9ebacd84-439f-4a77-e1dc-694ab155539b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF:  30.107267737625925\n",
      "LSI:  29.800199080327467\n"
     ]
    }
   ],
   "source": [
    "# Compare LSI and NMF\n",
    "\n",
    "nmf_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(W_nmf_train_reduced, H_nmf_train_reduced), 'fro')\n",
    "lsi_val = np.linalg.norm(X_lemmatized_train_tfidf - np.matmul(X_lsi_train_reduced, Y_lsi_train_reduced), 'fro')\n",
    "\n",
    "print('NMF: ', nmf_val)\n",
    "print('LSI: ', lsi_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic Classifier:\n",
    "\n",
    "5-Fold Cross-Validation on dimension-reduced-by-SVD training data to find best regulation strength in range $\\{10^{k} | -3 \\leq k \\leq 3, k \\in \\mathbb{Z} \\}$ for logistic regression with L1 regularization and logistic regression L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGNn6K_exvyB"
   },
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5NkfS2HZxvyB"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Train a Naive Bayes Gaussian classifier on the reduced TFIDF training set from problem 3\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = GaussianNB().fit(W_nmf_train_reduced, twenty_train.target)\n",
    "clf2 = MultinomialNB().fit(W_nmf_train_reduced, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQN9jIzvxvyE",
    "outputId": "6308ec42-fcf7-4348-a564-562c886cda29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics =? comp.graphics\n",
      "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
      "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
      "comp.graphics =? comp.sys.mac.hardware\n",
      "comp.graphics =? comp.sys.mac.hardware\n",
      "...\n",
      "\n",
      "Accuracy of NB Gaussian: 0.5568475452196382\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Generate predictions for test set\n",
    "\n",
    "predicted = clf.predict(W_nmf_test_reduced)\n",
    "correct = 0\n",
    "for i, category in enumerate(predicted):\n",
    "    if category == twenty_test.target[i]:\n",
    "        correct += 1\n",
    "    if i < 5:\n",
    "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
    "    elif i == 5:\n",
    "        print('...\\n')\n",
    "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0IChMnAxvyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics =? comp.graphics\n",
      "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
      "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
      "comp.graphics =? comp.sys.mac.hardware\n",
      "comp.sys.mac.hardware =? comp.sys.mac.hardware\n",
      "...\n",
      "\n",
      "Accuracy of NB Gaussian: 0.5775193798449613\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "# Generate predictions for test set\n",
    "\n",
    "predicted = clf2.predict(W_nmf_test_reduced)\n",
    "correct = 0\n",
    "for i, category in enumerate(predicted):\n",
    "    if category == twenty_test.target[i]:\n",
    "        correct += 1\n",
    "    if i < 5:\n",
    "        print('{} =? {}'.format(twenty_test.target_names[category], twenty_test.target_names[twenty_test.target[i]]))\n",
    "    elif i == 5:\n",
    "        print('...\\n')\n",
    "print('Accuracy of NB Gaussian: {}'.format(correct / W_nmf_test_reduced.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp2QQX5ByD0-"
   },
   "source": [
    "#### TEST"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "proj1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
